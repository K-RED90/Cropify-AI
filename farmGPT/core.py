from .prompts import (
    INITIAL_MESSAGE_VALIDATION,
    EVALUATION_PROMPT,
    RAG_PROMPT,
    FALLBACK_PROMPT,
)
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import Literal, Optional, Type
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain.tools.render import render_text_description_and_args
from langchain_core.tools import BaseTool
from datetime import datetime


class RouterSchema(BaseModel):
    """Use this tool to route farm-related queries to the appropriate AI assistant."""

    route: Literal["farm_query", "weather", "other"] = Field(
        ...,
        description="The route to the appropriate AI assistant based on the farmer's query.",
    )


def input_validator(llm: BaseChatModel, system_template: Optional[str] = None):
    prompt_template = system_template or INITIAL_MESSAGE_VALIDATION
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", prompt_template),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ]
    )
    chain = prompt | llm.with_structured_output(RouterSchema, method="function_calling")
    return chain


def farm_chain(
    llm: BaseChatModel,
    system_prompt: str,
    tools: Optional[list[BaseTool]] = None,
    tool_choice: Optional[str] = "auto",
):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            MessagesPlaceholder(variable_name="messages", Optional=True),
        ]
    )
    return prompt | (
        llm if not tools else llm.bind_tools(tools=tools, tool_choice=tool_choice)
    )


def evaluator(
    llm: BaseChatModel,
    tool: BaseTool | BaseTool,
    prompt_template: Optional[str] = None,
):
    prompt_template = prompt_template or EVALUATION_PROMPT
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm.bind_tools(tools=[tool], tool_choice=tool.__name__)
    return chain


class OutputSchema(BaseModel):
    """Your answer to the farm-related query should be in this format."""

    response: str = Field(
        ...,
        description="The response generated by the AI assistant for the given query.",
    )
    sources: list[str] = Field(
        ...,
        description="urls of the sources used to generate the response.",
    )


def rag_agent(llm: BaseChatModel, system_prompt: Optional[str] = None):
    system_prompt = system_prompt or RAG_PROMPT
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "Context: {context}\n\nQuery: {query}"),
        ]
    )
    return (
        prompt
        | llm  # .bind_tools(tools=[OutputSchema], tool_choice="OutputSchema")
        # | JsonOutputToolsParser()
    )


def fallback_response(
    llm: BaseChatModel,
    system_prompt: Optional[str] = None,
    tools: Optional[list[BaseTool]] = None,
):
    system_prompt = system_prompt or FALLBACK_PROMPT
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    return prompt | (llm if not tools else llm.bind_tools(tools=tools))


class Recommendation(BaseModel):
    description: str = Field(
        ..., description="Description of the treatment recommendations"
    )
    type: str = Field(
        ...,
        description="Type of recommendation (e.g., 'pesticide', 'organic treatment', 'cultural practice')",
    )


class Pest(BaseModel):
    pest: str = Field(..., description="name of the pest")
    description: str = Field(
        ...,
        description="Description of the pest, including physical characteristics, behavior, and crops affected",
    )
    potential_damage: str = Field(
        ..., description="Potential damage caused by the pest"
    )


class Disease(BaseModel):
    disease: str = Field(..., description="Name of the disease")
    symptoms: str = Field(..., description="Symptoms of the disease")
    potential_impact: str = Field(
        ..., description="Potential impact of the disease on crop yield and quality"
    )


class PestOrDisease(BaseModel):
    """Use this schema to provide information about a pest, crop, or other issue identified in an image."""

    label: Literal["pest", "crop_disease", "other_image"] = Field(
        ...,
        description="The type of issue identified in the image. If the image shows a pest, the label should be 'pest'. If the image shows a crop_disease/crop, the label should be 'crop_disease'. If the image does not show a pest or crop disease/crop, the label should be 'other_image'.",
    )


def extract_json_str(s):
    start = s.find("{")
    end = s.rfind("}") + 1
    json_str = s[start:end]
    return json_str


def pest_and_disease_tool(
    llm: BaseChatModel, img_base64, prompt_template: str, schema: Type[BaseModel]
):
    """This function can be used to classify uploaded images as pests, diseases, or other issues and can also be used to provide recommendations for controlling pests or diseases."""
    pydantic_parser = PydanticOutputParser(pydantic_object=schema)
    format_instructions = pydantic_parser.get_format_instructions()
    messages = [
        HumanMessage(
            content=[
                {
                    "type": "text",
                    "text": prompt_template.format(
                        format_instructions=format_instructions
                    ),
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"},
                },
            ]
        )
    ]
    return (
        llm | StrOutputParser() | RunnableLambda(extract_json_str) | pydantic_parser
    ).invoke(messages)


def meteorologist_chain(llm, system_prompt: str, tools: list[BaseTool]):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    return (
        RunnablePassthrough.assign(
            location="Accra,GH",
            date_time=datetime.now().strftime("%A, %B %d, %Y %H:%M:%S"),
        )
        | prompt
        | (llm if not tools else llm.bind_tools(tools=tools))
    )


def specialist_chain(
    llm: BaseChatModel,
    system_prompt: str,
    tools: Optional[list[BaseTool]] = None,
    tool_choice: Optional[str] = "auto",
):
    prompt = ChatPromptTemplate.from_template(system_prompt)
    return prompt | (
        llm if not tools else llm.bind_tools(tools=tools, tool_choice=tool_choice)
    )
