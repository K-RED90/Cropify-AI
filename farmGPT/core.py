from farmGPT.prompts import (
    INITIAL_MESSAGE_VALIDATION,
    FARM_LLM_SYSTEM_PROMPT,
    EVALUATION_PROMPT,
    RAG_PROMPT,
    FALLBACK_PROMPT
)
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import Literal, Optional
from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser


class ValidatorOutput(BaseModel):
    """Use this tool to validate the user's input"""

    is_farm_related: Literal["Yes", "No"] = Field(
        ..., description="Whether the given query is farm-related or not."
    )


def input_validator(llm: BaseChatModel, prompt_template: Optional[str] = None):
    prompt_template = prompt_template or INITIAL_MESSAGE_VALIDATION
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm.with_structured_output(
        ValidatorOutput, method="function_calling"
    )
    return chain


def farm_llm(llm: BaseChatModel, system_prompt: Optional[str] = None):
    system_prompt = system_prompt or FARM_LLM_SYSTEM_PROMPT
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_histoy"),
            ("user", "Input: {input}"),
        ]
    )
    return prompt | llm


class QueryContextEvaluation(BaseModel):
    """Use this tool to evaluate the relevance of text for answering farm-related queries"""

    decision: Literal["relevant", "not relevant"] = Field(
        ...,
        description="The relevance of the context for answering a farm-related query.",
    )


def search_content_evaluator(llm: BaseChatModel, prompt_template: Optional[str] = None):
    prompt_template = prompt_template or EVALUATION_PROMPT
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm.with_structured_output(
        QueryContextEvaluation, method="function_calling"
    )
    return chain


class OutputSchema(BaseModel):
    """Your answer to the farm-related query should be in this format."""

    response: str = Field(
        ...,
        description="The response generated by the AI assistant for the given query.",
    )
    sources: list[str] = Field(
        ...,
        description="urls of the sources used to generate the response.",
    )


def rag_agent(llm: BaseChatModel, system_prompt: Optional[str] = None):
    system_prompt = system_prompt or RAG_PROMPT
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "Context: {context}\n\nQuery: {query}"),
        ]
    )
    return (
        prompt
        | llm.bind_tools(tools=[OutputSchema], tool_choice="OutputSchema")
        | JsonOutputToolsParser()
    )


def fallback_response(llm: BaseChatModel, system_prompt: Optional[str] = None):
    system_prompt = system_prompt or FALLBACK_PROMPT
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "Input: {input}")
    ])
    return prompt | llm